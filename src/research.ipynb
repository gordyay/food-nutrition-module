{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cf177d6",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "f166f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/'\n",
    "api_key='ehlVSIKSMiugJZHlii3sU7OKCexe4MIlnYenGric'\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdaf62",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd70e2d",
   "metadata": {},
   "source": [
    "## Take  a look on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "e04c1794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: 17736\n",
      "rating: 8\n",
      "calories: 1858\n",
      "protein: 282\n",
      "fat: 326\n",
      "sodium: 2434\n",
      "#cakeweek: 2\n",
      "#wasteless: 2\n",
      "22-minute meals: 2\n",
      "3-ingredient recipes: 2\n",
      "30 days of groceries: 2\n",
      "advance prep required: 2\n",
      "alabama: 2\n",
      "alaska: 2\n",
      "alcoholic: 2\n",
      "almond: 2\n",
      "amaretto: 2\n",
      "anchovy: 2\n",
      "anise: 2\n",
      "anniversary: 2\n",
      "anthony bourdain: 2\n",
      "aperitif: 2\n",
      "appetizer: 2\n",
      "apple: 2\n",
      "apple juice: 2\n",
      "apricot: 2\n",
      "arizona: 2\n",
      "artichoke: 2\n",
      "arugula: 2\n",
      "asian pear: 2\n",
      "asparagus: 2\n",
      "aspen: 2\n",
      "atlanta: 2\n",
      "australia: 2\n",
      "avocado: 2\n",
      "back to school: 2\n",
      "backyard bbq: 2\n",
      "bacon: 2\n",
      "bake: 2\n",
      "banana: 2\n",
      "barley: 2\n",
      "basil: 2\n",
      "bass: 2\n",
      "bastille day: 2\n",
      "bean: 2\n",
      "beef: 2\n",
      "beef rib: 2\n",
      "beef shank: 2\n",
      "beef tenderloin: 2\n",
      "beer: 2\n",
      "beet: 2\n",
      "bell pepper: 2\n",
      "berry: 2\n",
      "beverly hills: 2\n",
      "birthday: 2\n",
      "biscuit: 2\n",
      "bitters: 2\n",
      "blackberry: 2\n",
      "blender: 2\n",
      "blue cheese: 2\n",
      "blueberry: 2\n",
      "boil: 2\n",
      "bok choy: 2\n",
      "bon appétit: 2\n",
      "bon app��tit: 2\n",
      "boston: 2\n",
      "bourbon: 2\n",
      "braise: 2\n",
      "bran: 2\n",
      "brandy: 2\n",
      "bread: 2\n",
      "breadcrumbs: 2\n",
      "breakfast: 2\n",
      "brie: 2\n",
      "brine: 2\n",
      "brisket: 2\n",
      "broccoli: 2\n",
      "broccoli rabe: 2\n",
      "broil: 2\n",
      "brooklyn: 2\n",
      "brown rice: 2\n",
      "brownie: 2\n",
      "brunch: 2\n",
      "brussel sprout: 2\n",
      "buffalo: 2\n",
      "buffet: 2\n",
      "bulgaria: 2\n",
      "bulgur: 2\n",
      "burrito: 2\n",
      "butter: 2\n",
      "buttermilk: 2\n",
      "butternut squash: 2\n",
      "butterscotch/caramel: 2\n",
      "cabbage: 2\n",
      "cake: 2\n",
      "california: 2\n",
      "calvados: 2\n",
      "cambridge: 2\n",
      "campari: 2\n",
      "camping: 2\n",
      "canada: 2\n",
      "candy: 2\n",
      "candy thermometer: 2\n",
      "cantaloupe: 2\n",
      "capers: 2\n",
      "caraway: 2\n",
      "cardamom: 2\n",
      "carrot: 2\n",
      "cashew: 2\n",
      "casserole/gratin: 2\n",
      "cauliflower: 2\n",
      "caviar: 2\n",
      "celery: 2\n",
      "chambord: 2\n",
      "champagne: 2\n",
      "chard: 2\n",
      "chartreuse: 2\n",
      "cheddar: 2\n",
      "cheese: 2\n",
      "cherry: 2\n",
      "chestnut: 2\n",
      "chicago: 2\n",
      "chicken: 2\n",
      "chickpea: 2\n",
      "chile: 2\n",
      "chile pepper: 2\n",
      "chili: 2\n",
      "chill: 2\n",
      "chive: 2\n",
      "chocolate: 2\n",
      "christmas: 2\n",
      "christmas eve: 2\n",
      "cilantro: 2\n",
      "cinco de mayo: 2\n",
      "cinnamon: 2\n",
      "citrus: 2\n",
      "clam: 2\n",
      "clove: 2\n",
      "cobbler/crumble: 2\n",
      "cocktail: 2\n",
      "cocktail party: 2\n",
      "coconut: 2\n",
      "cod: 2\n",
      "coffee: 2\n",
      "coffee grinder: 2\n",
      "cognac/armagnac: 2\n",
      "collard greens: 2\n",
      "colorado: 2\n",
      "columbus: 2\n",
      "condiment: 2\n",
      "condiment/spread: 2\n",
      "connecticut: 2\n",
      "cook like a diner: 2\n",
      "cookbook critic: 2\n",
      "cookie: 2\n",
      "cookies: 2\n",
      "coriander: 2\n",
      "corn: 2\n",
      "cornmeal: 2\n",
      "costa mesa: 2\n",
      "cottage cheese: 2\n",
      "couscous: 2\n",
      "crab: 2\n",
      "cranberry: 2\n",
      "cranberry sauce: 2\n",
      "cream cheese: 2\n",
      "créme de cacao: 2\n",
      "crêpe: 2\n",
      "cr��me de cacao: 2\n",
      "cuba: 2\n",
      "cucumber: 2\n",
      "cumin: 2\n",
      "cupcake: 2\n",
      "currant: 2\n",
      "curry: 2\n",
      "custard: 2\n",
      "dairy: 2\n",
      "dairy free: 2\n",
      "dallas: 2\n",
      "date: 2\n",
      "deep-fry: 2\n",
      "denver: 2\n",
      "dessert: 2\n",
      "digestif: 2\n",
      "dill: 2\n",
      "dinner: 2\n",
      "dip: 2\n",
      "diwali: 2\n",
      "dominican republic: 2\n",
      "dorie greenspan: 2\n",
      "double boiler: 2\n",
      "dried fruit: 2\n",
      "drink: 2\n",
      "drinks: 2\n",
      "duck: 2\n",
      "easter: 2\n",
      "eau de vie: 2\n",
      "edible gift: 2\n",
      "egg: 2\n",
      "egg nog: 2\n",
      "eggplant: 2\n",
      "egypt: 2\n",
      "emeril lagasse: 2\n",
      "endive: 2\n",
      "engagement party: 2\n",
      "england: 2\n",
      "entertaining: 2\n",
      "epi + ushg: 2\n",
      "epi loves the microwave: 2\n",
      "escarole: 2\n",
      "fall: 2\n",
      "family reunion: 2\n",
      "fat free: 2\n",
      "father's day: 2\n",
      "fennel: 2\n",
      "feta: 2\n",
      "fig: 2\n",
      "fish: 2\n",
      "flaming hot summer: 2\n",
      "flat bread: 2\n",
      "florida: 2\n",
      "fontina: 2\n",
      "food processor: 2\n",
      "fortified wine: 2\n",
      "fourth of july: 2\n",
      "france: 2\n",
      "frangelico: 2\n",
      "frankenrecipe: 2\n",
      "freeze/chill: 2\n",
      "freezer food: 2\n",
      "friendsgiving: 2\n",
      "frittata: 2\n",
      "fritter: 2\n",
      "frozen dessert: 2\n",
      "fruit: 2\n",
      "fruit juice: 2\n",
      "fry: 2\n",
      "game: 2\n",
      "garlic: 2\n",
      "georgia: 2\n",
      "germany: 2\n",
      "gin: 2\n",
      "ginger: 2\n",
      "goat cheese: 2\n",
      "goose: 2\n",
      "gouda: 2\n",
      "gourmet: 2\n",
      "graduation: 2\n",
      "grains: 2\n",
      "grand marnier: 2\n",
      "granola: 2\n",
      "grape: 2\n",
      "grapefruit: 2\n",
      "grappa: 2\n",
      "green bean: 2\n",
      "green onion/scallion: 2\n",
      "grill: 2\n",
      "grill/barbecue: 2\n",
      "ground beef: 2\n",
      "ground lamb: 2\n",
      "guam: 2\n",
      "guava: 2\n",
      "haiti: 2\n",
      "halibut: 2\n",
      "halloween: 2\n",
      "ham: 2\n",
      "hamburger: 2\n",
      "hanukkah: 2\n",
      "harpercollins: 2\n",
      "hawaii: 2\n",
      "hazelnut: 2\n",
      "healdsburg: 2\n",
      "healthy: 2\n",
      "herb: 2\n",
      "high fiber: 2\n",
      "hollywood: 2\n",
      "hominy/cornmeal/masa: 2\n",
      "honey: 2\n",
      "honeydew: 2\n",
      "hors d'oeuvre: 2\n",
      "horseradish: 2\n",
      "hot drink: 2\n",
      "hot pepper: 2\n",
      "house & garden: 2\n",
      "house cocktail: 2\n",
      "houston: 2\n",
      "hummus: 2\n",
      "ice cream: 2\n",
      "ice cream machine: 2\n",
      "iced coffee: 2\n",
      "iced tea: 2\n",
      "idaho: 2\n",
      "illinois: 2\n",
      "indiana: 2\n",
      "iowa: 2\n",
      "ireland: 2\n",
      "israel: 2\n",
      "italy: 2\n",
      "jalapeño: 2\n",
      "jam or jelly: 2\n",
      "jamaica: 2\n",
      "japan: 2\n",
      "jerusalem artichoke: 2\n",
      "juicer: 2\n",
      "jícama: 2\n",
      "kahlúa: 2\n",
      "kale: 2\n",
      "kansas: 2\n",
      "kansas city: 2\n",
      "kentucky: 2\n",
      "kentucky derby: 2\n",
      "kid-friendly: 2\n",
      "kidney friendly: 2\n",
      "kirsch: 2\n",
      "kitchen olympics: 2\n",
      "kiwi: 2\n",
      "kosher: 2\n",
      "kosher for passover: 2\n",
      "kumquat: 2\n",
      "kwanzaa: 2\n",
      "labor day: 2\n",
      "lamb: 2\n",
      "lamb chop: 2\n",
      "lamb shank: 2\n",
      "lancaster: 2\n",
      "las vegas: 2\n",
      "lasagna: 2\n",
      "leafy green: 2\n",
      "leek: 2\n",
      "legume: 2\n",
      "lemon: 2\n",
      "lemon juice: 2\n",
      "lemongrass: 2\n",
      "lentil: 2\n",
      "lettuce: 2\n",
      "lima bean: 2\n",
      "lime: 2\n",
      "lime juice: 2\n",
      "lingonberry: 2\n",
      "liqueur: 2\n",
      "lobster: 2\n",
      "london: 2\n",
      "long beach: 2\n",
      "los angeles: 2\n",
      "louisiana: 2\n",
      "louisville: 2\n",
      "low cal: 2\n",
      "low carb: 2\n",
      "low cholesterol: 2\n",
      "low fat: 2\n",
      "low sodium: 2\n",
      "low sugar: 2\n",
      "low/no sugar: 2\n",
      "lunar new year: 2\n",
      "lunch: 2\n",
      "lychee: 2\n",
      "macadamia nut: 2\n",
      "macaroni and cheese: 2\n",
      "maine: 2\n",
      "mandoline: 2\n",
      "mango: 2\n",
      "maple syrup: 2\n",
      "mardi gras: 2\n",
      "margarita: 2\n",
      "marinade: 2\n",
      "marinate: 2\n",
      "marsala: 2\n",
      "marscarpone: 2\n",
      "marshmallow: 2\n",
      "martini: 2\n",
      "maryland: 2\n",
      "massachusetts: 2\n",
      "mayonnaise: 2\n",
      "meat: 2\n",
      "meatball: 2\n",
      "meatloaf: 2\n",
      "melon: 2\n",
      "mexico: 2\n",
      "mezcal: 2\n",
      "miami: 2\n",
      "michigan: 2\n",
      "microwave: 2\n",
      "midori: 2\n",
      "milk/cream: 2\n",
      "minneapolis: 2\n",
      "minnesota: 2\n",
      "mint: 2\n",
      "mississippi: 2\n",
      "missouri: 2\n",
      "mixer: 2\n",
      "molasses: 2\n",
      "monterey jack: 2\n",
      "mortar and pestle: 2\n",
      "mother's day: 2\n",
      "mozzarella: 2\n",
      "muffin: 2\n",
      "mushroom: 2\n",
      "mussel: 2\n",
      "mustard: 2\n",
      "mustard greens: 2\n",
      "nancy silverton: 2\n",
      "nebraska: 2\n",
      "nectarine: 2\n",
      "new hampshire: 2\n",
      "new jersey: 2\n",
      "new mexico: 2\n",
      "new orleans: 2\n",
      "new year's day: 2\n",
      "new year's eve: 2\n",
      "new york: 2\n",
      "no meat, no problem: 2\n",
      "no sugar added: 2\n",
      "no-cook: 2\n",
      "non-alcoholic: 2\n",
      "noodle: 2\n",
      "north carolina: 2\n",
      "nut: 2\n",
      "nutmeg: 2\n",
      "oat: 2\n",
      "oatmeal: 2\n",
      "octopus: 2\n",
      "ohio: 2\n",
      "oklahoma: 2\n",
      "okra: 2\n",
      "oktoberfest: 2\n",
      "olive: 2\n",
      "omelet: 2\n",
      "one-pot meal: 2\n",
      "onion: 2\n",
      "orange: 2\n",
      "orange juice: 2\n",
      "oregano: 2\n",
      "oregon: 2\n",
      "organic: 2\n",
      "orzo: 2\n",
      "oscars: 2\n",
      "oyster: 2\n",
      "pacific palisades: 2\n",
      "paleo: 2\n",
      "pan-fry: 2\n",
      "pancake: 2\n",
      "papaya: 2\n",
      "paprika: 2\n",
      "parade: 2\n",
      "paris: 2\n",
      "parmesan: 2\n",
      "parsley: 2\n",
      "parsnip: 2\n",
      "party: 2\n",
      "pasadena: 2\n",
      "passion fruit: 2\n",
      "passover: 2\n",
      "pasta: 2\n",
      "pasta maker: 2\n",
      "pastry: 2\n",
      "pea: 2\n",
      "peach: 2\n",
      "peanut: 2\n",
      "peanut butter: 2\n",
      "peanut free: 2\n",
      "pear: 2\n",
      "pecan: 2\n",
      "pennsylvania: 2\n",
      "pepper: 2\n",
      "pernod: 2\n",
      "persian new year: 2\n",
      "persimmon: 2\n",
      "peru: 2\n",
      "pescatarian: 2\n",
      "philippines: 2\n",
      "phyllo/puff pastry dough: 2\n",
      "pickles: 2\n",
      "picnic: 2\n",
      "pie: 2\n",
      "pine nut: 2\n",
      "pineapple: 2\n",
      "pistachio: 2\n",
      "pittsburgh: 2\n",
      "pizza: 2\n",
      "plantain: 2\n",
      "plum: 2\n",
      "poach: 2\n",
      "poblano: 2\n",
      "poker/game night: 2\n",
      "pomegranate: 2\n",
      "pomegranate juice: 2\n",
      "poppy: 2\n",
      "pork: 2\n",
      "pork chop: 2\n",
      "pork rib: 2\n",
      "pork tenderloin: 2\n",
      "port: 2\n",
      "portland: 2\n",
      "pot pie: 2\n",
      "potato: 2\n",
      "potato salad: 2\n",
      "potluck: 2\n",
      "poultry: 2\n",
      "poultry sausage: 2\n",
      "pressure cooker: 2\n",
      "prosciutto: 2\n",
      "providence: 2\n",
      "prune: 2\n",
      "pumpkin: 2\n",
      "punch: 2\n",
      "purim: 2\n",
      "quail: 2\n",
      "quiche: 2\n",
      "quick & easy: 2\n",
      "quick and healthy: 2\n",
      "quince: 2\n",
      "quinoa: 2\n",
      "rabbit: 2\n",
      "rack of lamb: 2\n",
      "radicchio: 2\n",
      "radish: 2\n",
      "raisin: 2\n",
      "ramadan: 2\n",
      "ramekin: 2\n",
      "raspberry: 2\n",
      "raw: 2\n",
      "red wine: 2\n",
      "rhode island: 2\n",
      "rhubarb: 2\n",
      "rice: 2\n",
      "ricotta: 2\n",
      "roast: 2\n",
      "root vegetable: 2\n",
      "rosemary: 2\n",
      "rosh hashanah/yom kippur: 2\n",
      "rosé: 2\n",
      "rub: 2\n",
      "rum: 2\n",
      "rutabaga: 2\n",
      "rye: 2\n",
      "saffron: 2\n",
      "sage: 2\n",
      "sake: 2\n",
      "salad: 2\n",
      "salad dressing: 2\n",
      "salmon: 2\n",
      "salsa: 2\n",
      "san francisco: 2\n",
      "sandwich: 2\n",
      "sandwich theory: 2\n",
      "sangria: 2\n",
      "santa monica: 2\n",
      "sardine: 2\n",
      "sauce: 2\n",
      "sausage: 2\n",
      "sauté: 2\n",
      "scallop: 2\n",
      "scotch: 2\n",
      "seafood: 2\n",
      "seattle: 2\n",
      "seed: 2\n",
      "self: 2\n",
      "semolina: 2\n",
      "sesame: 2\n",
      "sesame oil: 2\n",
      "shallot: 2\n",
      "shavuot: 2\n",
      "shellfish: 2\n",
      "sherry: 2\n",
      "shower: 2\n",
      "shrimp: 2\n",
      "side: 2\n",
      "simmer: 2\n",
      "skewer: 2\n",
      "slow cooker: 2\n",
      "smoker: 2\n",
      "smoothie: 2\n",
      "snapper: 2\n",
      "sorbet: 2\n",
      "soufflé/meringue: 2\n",
      "soup/stew: 2\n",
      "sour cream: 2\n",
      "sourdough: 2\n",
      "south carolina: 2\n",
      "soy: 2\n",
      "soy free: 2\n",
      "soy sauce: 2\n",
      "spain: 2\n",
      "sparkling wine: 2\n",
      "spice: 2\n",
      "spinach: 2\n",
      "spirit: 2\n",
      "spring: 2\n",
      "spritzer: 2\n",
      "squash: 2\n",
      "squid: 2\n",
      "st. louis: 2\n",
      "st. patrick's day: 2\n",
      "steak: 2\n",
      "steam: 2\n",
      "stew: 2\n",
      "stir-fry: 2\n",
      "stock: 2\n",
      "strawberry: 2\n",
      "stuffing/dressing: 2\n",
      "sugar conscious: 2\n",
      "sugar snap pea: 2\n",
      "sukkot: 2\n",
      "summer: 2\n",
      "super bowl: 2\n",
      "suzanne goin: 2\n",
      "sweet potato/yam: 2\n",
      "swiss cheese: 2\n",
      "switzerland: 2\n",
      "swordfish: 2\n",
      "taco: 2\n",
      "tailgating: 2\n",
      "tamarind: 2\n",
      "tangerine: 2\n",
      "tapioca: 2\n",
      "tarragon: 2\n",
      "tart: 2\n",
      "tea: 2\n",
      "tennessee: 2\n",
      "tequila: 2\n",
      "tested & improved: 2\n",
      "texas: 2\n",
      "thanksgiving: 2\n",
      "thyme: 2\n",
      "tilapia: 2\n",
      "tofu: 2\n",
      "tomatillo: 2\n",
      "tomato: 2\n",
      "tortillas: 2\n",
      "tree nut: 2\n",
      "tree nut free: 2\n",
      "triple sec: 2\n",
      "tropical fruit: 2\n",
      "trout: 2\n",
      "tuna: 2\n",
      "turnip: 2\n",
      "utah: 2\n",
      "valentine's day: 2\n",
      "vanilla: 2\n",
      "veal: 2\n",
      "vegan: 2\n",
      "vegetable: 2\n",
      "vegetarian: 2\n",
      "venison: 2\n",
      "vermont: 2\n",
      "vermouth: 2\n",
      "vinegar: 2\n",
      "virginia: 2\n",
      "vodka: 2\n",
      "waffle: 2\n",
      "walnut: 2\n",
      "wasabi: 2\n",
      "washington: 2\n",
      "washington, d.c.: 2\n",
      "watercress: 2\n",
      "watermelon: 2\n",
      "wedding: 2\n",
      "weelicious: 2\n",
      "west virginia: 2\n",
      "westwood: 2\n",
      "wheat/gluten-free: 2\n",
      "whiskey: 2\n",
      "white wine: 2\n",
      "whole wheat: 2\n",
      "wild rice: 2\n",
      "windsor: 2\n",
      "wine: 2\n",
      "winter: 2\n",
      "wisconsin: 2\n",
      "wok: 2\n",
      "yellow squash: 2\n",
      "yogurt: 2\n",
      "yonkers: 2\n",
      "yuca: 2\n",
      "zucchini: 2\n",
      "cookbooks: 2\n",
      "leftovers: 2\n",
      "snack: 2\n",
      "snack week: 2\n",
      "turkey: 2\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv(f\"{data_path}epi_r.csv\")\n",
    "for name, count in df.nunique().items():\n",
    "    print(f\"{name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a3825",
   "metadata": {},
   "source": [
    "The dataset have target feature:\n",
    "```rating```.\n",
    "\n",
    "There is also features: ```title, calories, protein, fat, sodium```\n",
    "\n",
    "\n",
    "\n",
    "Dataset have 2 categorical features after the One-Hot Encoding conversion.\n",
    "\n",
    "first categorical feature is:\n",
    "\n",
    "Ingredients: ```almond, anchovy, apple, avocado, bacon```\n",
    "\n",
    "second categorical feature is tags\n",
    "\n",
    "The tags may contain:\n",
    "* Recipe characteristics: ```#cakeweek, #wasteless, 22-minute meals, 3-ingredient recipes...```\n",
    "\n",
    "* Geographical affiliation: ```alabama, arizona, boston, france, italy...```\n",
    "\n",
    "* Dietary restrictions and features: ```dairy free, gluten free, vegan...```\n",
    "\n",
    "* Cooking methods: ```bake, boil, grill, slow cooker...```\n",
    "\n",
    "* Holidays and events: ```christmas, halloween, valentine's day...```\n",
    "\n",
    "* Machinery and tools: ```blender, food processor, pressure cooker, microwave...```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb48838",
   "metadata": {},
   "source": [
    "## Let's sort columns in the dataset, and get nutrients for ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3279b",
   "metadata": {},
   "source": [
    "### this is list of nutrients, what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "2ecb71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients_list = [\n",
    "    # Основные макронутриенты \n",
    "    \"Total lipid (fat)\",\n",
    "    \"Fatty acids, total saturated\",\n",
    "    \"Fatty acids, total monounsaturated\",\n",
    "    \"Fatty acids, total polyunsaturated\",\n",
    "    \"Fatty acids, total trans\",\n",
    "    \"Cholesterol\",\n",
    "    \"Carbohydrate, by difference\",\n",
    "    \"Sodium, Na\", \n",
    "    \"Fiber, total dietary\",\n",
    "    \"Protein\",\n",
    "    \"Sugars, Total\",  \n",
    "    \"Energy\", \n",
    "    \"Vitamin A, RAE\",  \n",
    "    \"Vitamin C, total ascorbic acid\", \n",
    "    \"Vitamin D (D2 + D3)\",\n",
    "    \"Vitamin E (alpha-tocopherol)\",\n",
    "    \"Vitamin K (phylloquinone)\",\n",
    "    \"Thiamin\",\n",
    "    \"Riboflavin\",\n",
    "    \"Niacin\",\n",
    "    \"Vitamin B-6\",\n",
    "    \"Folate, total\",\n",
    "    \"Vitamin B-12\",\n",
    "    \n",
    "    # Минералы\n",
    "    \"Calcium, Ca\",\n",
    "    \"Iron, Fe\",\n",
    "    \"Phosphorus, P\",\n",
    "    \"Magnesium, Mg\",\n",
    "    \"Zinc, Zn\",\n",
    "    \"Copper, Cu\",\n",
    "    \"Manganese, Mn\",\n",
    "    \"Selenium, Se\",\n",
    "    \"Potassium, K\",\n",
    "    \n",
    "    # Специфичные компоненты \n",
    "    \"Ash\",\n",
    "    \"Nitrogen\",\n",
    "    \"Water\",\n",
    "    \n",
    "    # Разные формы сахаров\n",
    "    \"Fructose\",\n",
    "    \"Glucose\",\n",
    "    \"Sucrose\",\n",
    "    \"Galactose\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be624830",
   "metadata": {},
   "source": [
    "### Funcs for primary sort ingredients and get cache for nutrients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "062b7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "def load_nutrients_cache(path_to_file):\n",
    "    try:\n",
    "        with open(path_to_file, 'r', encoding='utf-8') as f:\n",
    "            try:\n",
    "                return json.load(f)\n",
    "            except:\n",
    "                return {}\n",
    "    except:\n",
    "        return {}\n",
    "def save_to_cache(path_to_file, ingredient, nutrient_dict, cache):\n",
    "    cache[ingredient]=nutrient_dict\n",
    "    with open(path_to_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(cache, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def get_nutrients_info(ingredient):\n",
    "    ingredient_info=None\n",
    "    params = {\n",
    "        \"query\": f'\"{ingredient}\"',\n",
    "        \"api_key\": api_key,\n",
    "        \"dataType\": [\"Foundation\"]  \n",
    "    }\n",
    "    cache=load_nutrients_cache(f\"{data_path}ingridient_nutr_cache.json\")\n",
    "    non_ingredient_cache=load_nutrients_cache(f\"{data_path}not_ingridient_cache.json\")\n",
    "    if ingredient in cache:\n",
    "        return cache[ingredient]\n",
    "    elif ingredient not in non_ingredient_cache:\n",
    "        response = requests.get('https://api.nal.usda.gov/fdc/v1/foods/search', params=params)\n",
    "        if response.status_code == 200:\n",
    "            ingredient_info = response.json().get('foods')\n",
    "            if ingredient_info and ingredient_info[0].get('score') < 200:\n",
    "                ingredient_info=None\n",
    "            if ingredient_info:\n",
    "                ingredient_info=ingredient_info[0].get('foodNutrients')\n",
    "                nutrient_df=pd.json_normalize(ingredient_info)[['nutrientName','unitName','value']]\n",
    "                nutrient_dict=nutrient_df[nutrient_df['nutrientName'].isin(nutrients_list)].to_dict()       \n",
    "                save_to_cache(f\"{data_path}ingridient_nutr_cache.json\",ingredient, nutrient_dict, cache)\n",
    "    if not ingredient_info:\n",
    "        save_to_cache(f\"{data_path}not_ingridient_cache.json\",ingredient, ingredient_info, non_ingredient_cache)\n",
    "    return ingredient_info\n",
    "def is_ingredient(ingredient):\n",
    "    data=get_nutrients_info(ingredient)\n",
    "    return True if data else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "87648796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a list of possible ingredients\n",
      "['fat', 'sodium', 'almond', 'apple', 'apple juice', 'apricot', 'arugula', 'asparagus', 'avocado', 'bacon', 'banana', 'barley', 'bean', 'beef', 'beef tenderloin', 'beet', 'blackberry', 'blueberry', 'boil', 'bok choy', 'braise', 'bran', 'bread', 'breakfast', 'broccoli', 'broil', 'brussel sprout', 'bulgur', 'butter', 'buttermilk', 'cabbage', 'cantaloupe', 'carrot', 'cashew', 'cauliflower', 'celery', 'cheddar', 'cherry', 'chestnut', 'chicken', 'chickpea', 'cobbler/crumble', 'coconut', 'cod', 'cookie', 'cookies', 'corn', 'cottage cheese', 'crab', 'cranberry', 'cream cheese', 'cucumber', 'dill', 'egg', 'eggplant', 'fat free', 'feta', 'fig', 'fish', 'garlic', 'grains', 'grape', 'grapefruit', 'green onion/scallion', 'ham', 'hazelnut', 'hominy/cornmeal/masa', 'honeydew', 'hummus', 'kale', 'kiwi', 'kosher', 'lamb', 'leek', 'lentil', 'lettuce', 'low fat', 'low/no sugar', 'macadamia nut', 'mango', 'meat', 'melon', 'milk/cream', 'monterey jack', 'mozzarella', 'mushroom', 'mustard', 'nectarine', 'no sugar added', 'no-cook', 'nut', 'oat', 'oatmeal', 'olive', 'onion', 'orange', 'orange juice', 'oyster', 'parmesan', 'pasta', 'pastry', 'pea', 'peach', 'peanut', 'peanut butter', 'pear', 'pecan', 'pepper', 'pickles', 'pie', 'pine nut', 'pineapple', 'pistachio', 'plantain', 'plum', 'pomegranate', 'pork', 'pork chop', 'potato', 'prune', 'pumpkin', 'quinoa', 'raisin', 'raspberry', 'raw', 'rice', 'ricotta', 'rutabaga', 'rye', 'salmon', 'salsa', 'sausage', 'seed', 'semolina', 'sesame', 'shallot', 'shrimp', 'soy', 'spinach', 'squash', 'steak', 'strawberry', 'stuffing/dressing', 'summer', 'sweet potato/yam', 'tart', 'tilapia', 'tomatillo', 'tomato', 'tuna', 'walnut', 'wheat/gluten-free', 'whole wheat', 'wild rice', 'winter', 'yogurt', 'zucchini', 'turkey']\n"
     ]
    }
   ],
   "source": [
    "features =[]\n",
    "for column in df.columns:\n",
    "    for ingredients in column.split(\"/\"):\n",
    "        if is_ingredient(ingredients):\n",
    "            features.append(column)\n",
    "            break\n",
    "print(\"a list of possible ingredients\")\n",
    "print (features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "138b1bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a list of possible non ingredients\n",
      "['title', 'rating', 'calories', 'protein', '#cakeweek', '#wasteless', '22-minute meals', '3-ingredient recipes', '30 days of groceries', 'advance prep required', 'alabama', 'alaska', 'alcoholic', 'amaretto', 'anchovy', 'anise', 'anniversary', 'anthony bourdain', 'aperitif', 'appetizer', 'arizona', 'artichoke', 'asian pear', 'aspen', 'atlanta', 'australia', 'back to school', 'backyard bbq', 'bake', 'basil', 'bass', 'bastille day', 'beef rib', 'beef shank', 'beer', 'bell pepper', 'berry', 'beverly hills', 'birthday', 'biscuit', 'bitters', 'blender', 'blue cheese', 'bon appétit', 'bon app��tit', 'boston', 'bourbon', 'brandy', 'breadcrumbs', 'brie', 'brine', 'brisket', 'broccoli rabe', 'brooklyn', 'brown rice', 'brownie', 'brunch', 'buffalo', 'buffet', 'bulgaria', 'burrito', 'butternut squash', 'butterscotch/caramel', 'cake', 'california', 'calvados', 'cambridge', 'campari', 'camping', 'canada', 'candy', 'candy thermometer', 'capers', 'caraway', 'cardamom', 'casserole/gratin', 'caviar', 'chambord', 'champagne', 'chard', 'chartreuse', 'cheese', 'chicago', 'chile', 'chile pepper', 'chili', 'chill', 'chive', 'chocolate', 'christmas', 'christmas eve', 'cilantro', 'cinco de mayo', 'cinnamon', 'citrus', 'clam', 'clove', 'cobbler/crumble', 'cocktail', 'cocktail party', 'coffee', 'coffee grinder', 'cognac/armagnac', 'collard greens', 'colorado', 'columbus', 'condiment', 'condiment/spread', 'connecticut', 'cook like a diner', 'cookbook critic', 'coriander', 'cornmeal', 'costa mesa', 'couscous', 'cranberry sauce', 'créme de cacao', 'crêpe', 'cr��me de cacao', 'cuba', 'cumin', 'cupcake', 'currant', 'curry', 'custard', 'dairy', 'dairy free', 'dallas', 'date', 'deep-fry', 'denver', 'dessert', 'digestif', 'dinner', 'dip', 'diwali', 'dominican republic', 'dorie greenspan', 'double boiler', 'dried fruit', 'drink', 'drinks', 'duck', 'easter', 'eau de vie', 'edible gift', 'egg nog', 'egypt', 'emeril lagasse', 'endive', 'engagement party', 'england', 'entertaining', 'epi + ushg', 'epi loves the microwave', 'escarole', 'fall', 'family reunion', \"father's day\", 'fennel', 'flaming hot summer', 'flat bread', 'florida', 'fontina', 'food processor', 'fortified wine', 'fourth of july', 'france', 'frangelico', 'frankenrecipe', 'freeze/chill', 'freezer food', 'friendsgiving', 'frittata', 'fritter', 'frozen dessert', 'fruit', 'fruit juice', 'fry', 'game', 'georgia', 'germany', 'gin', 'ginger', 'goat cheese', 'goose', 'gouda', 'gourmet', 'graduation', 'grand marnier', 'granola', 'grappa', 'green bean', 'grill', 'grill/barbecue', 'ground beef', 'ground lamb', 'guam', 'guava', 'haiti', 'halibut', 'halloween', 'hamburger', 'hanukkah', 'harpercollins', 'hawaii', 'healdsburg', 'healthy', 'herb', 'high fiber', 'hollywood', 'hominy/cornmeal/masa', 'honey', \"hors d'oeuvre\", 'horseradish', 'hot drink', 'hot pepper', 'house & garden', 'house cocktail', 'houston', 'ice cream', 'ice cream machine', 'iced coffee', 'iced tea', 'idaho', 'illinois', 'indiana', 'iowa', 'ireland', 'israel', 'italy', 'jalapeño', 'jam or jelly', 'jamaica', 'japan', 'jerusalem artichoke', 'juicer', 'jícama', 'kahlúa', 'kansas', 'kansas city', 'kentucky', 'kentucky derby', 'kid-friendly', 'kidney friendly', 'kirsch', 'kitchen olympics', 'kosher for passover', 'kumquat', 'kwanzaa', 'labor day', 'lamb chop', 'lamb shank', 'lancaster', 'las vegas', 'lasagna', 'leafy green', 'legume', 'lemon', 'lemon juice', 'lemongrass', 'lima bean', 'lime', 'lime juice', 'lingonberry', 'liqueur', 'lobster', 'london', 'long beach', 'los angeles', 'louisiana', 'louisville', 'low cal', 'low carb', 'low cholesterol', 'low sodium', 'low sugar', 'lunar new year', 'lunch', 'lychee', 'macaroni and cheese', 'maine', 'mandoline', 'maple syrup', 'mardi gras', 'margarita', 'marinade', 'marinate', 'marsala', 'marscarpone', 'marshmallow', 'martini', 'maryland', 'massachusetts', 'mayonnaise', 'meatball', 'meatloaf', 'mexico', 'mezcal', 'miami', 'michigan', 'microwave', 'midori', 'milk/cream', 'minneapolis', 'minnesota', 'mint', 'mississippi', 'missouri', 'mixer', 'molasses', 'mortar and pestle', \"mother's day\", 'muffin', 'mussel', 'mustard greens', 'nancy silverton', 'nebraska', 'new hampshire', 'new jersey', 'new mexico', 'new orleans', \"new year's day\", \"new year's eve\", 'new york', 'no meat, no problem', 'non-alcoholic', 'noodle', 'north carolina', 'nutmeg', 'octopus', 'ohio', 'oklahoma', 'okra', 'oktoberfest', 'omelet', 'one-pot meal', 'oregano', 'oregon', 'organic', 'orzo', 'oscars', 'pacific palisades', 'paleo', 'pan-fry', 'pancake', 'papaya', 'paprika', 'parade', 'paris', 'parsley', 'parsnip', 'party', 'pasadena', 'passion fruit', 'passover', 'pasta maker', 'peanut free', 'pennsylvania', 'pernod', 'persian new year', 'persimmon', 'peru', 'pescatarian', 'philippines', 'phyllo/puff pastry dough', 'picnic', 'pittsburgh', 'pizza', 'poach', 'poblano', 'poker/game night', 'pomegranate juice', 'poppy', 'pork rib', 'pork tenderloin', 'port', 'portland', 'pot pie', 'potato salad', 'potluck', 'poultry', 'poultry sausage', 'pressure cooker', 'prosciutto', 'providence', 'punch', 'purim', 'quail', 'quiche', 'quick & easy', 'quick and healthy', 'quince', 'rabbit', 'rack of lamb', 'radicchio', 'radish', 'ramadan', 'ramekin', 'red wine', 'rhode island', 'rhubarb', 'roast', 'root vegetable', 'rosemary', 'rosh hashanah/yom kippur', 'rosé', 'rub', 'rum', 'saffron', 'sage', 'sake', 'salad', 'salad dressing', 'san francisco', 'sandwich', 'sandwich theory', 'sangria', 'santa monica', 'sardine', 'sauce', 'sauté', 'scallop', 'scotch', 'seafood', 'seattle', 'self', 'sesame oil', 'shavuot', 'shellfish', 'sherry', 'shower', 'side', 'simmer', 'skewer', 'slow cooker', 'smoker', 'smoothie', 'snapper', 'sorbet', 'soufflé/meringue', 'soup/stew', 'sour cream', 'sourdough', 'south carolina', 'soy free', 'soy sauce', 'spain', 'sparkling wine', 'spice', 'spirit', 'spring', 'spritzer', 'squid', 'st. louis', \"st. patrick's day\", 'steam', 'stew', 'stir-fry', 'stock', 'stuffing/dressing', 'sugar conscious', 'sugar snap pea', 'sukkot', 'super bowl', 'suzanne goin', 'sweet potato/yam', 'swiss cheese', 'switzerland', 'swordfish', 'taco', 'tailgating', 'tamarind', 'tangerine', 'tapioca', 'tarragon', 'tea', 'tennessee', 'tequila', 'tested & improved', 'texas', 'thanksgiving', 'thyme', 'tofu', 'tortillas', 'tree nut', 'tree nut free', 'triple sec', 'tropical fruit', 'trout', 'turnip', 'utah', \"valentine's day\", 'vanilla', 'veal', 'vegan', 'vegetable', 'vegetarian', 'venison', 'vermont', 'vermouth', 'vinegar', 'virginia', 'vodka', 'waffle', 'wasabi', 'washington', 'washington, d.c.', 'watercress', 'watermelon', 'wedding', 'weelicious', 'west virginia', 'westwood', 'wheat/gluten-free', 'whiskey', 'white wine', 'windsor', 'wine', 'wisconsin', 'wok', 'yellow squash', 'yonkers', 'yuca', 'cookbooks', 'leftovers', 'snack', 'snack week']\n"
     ]
    }
   ],
   "source": [
    "non_ingreedients = []\n",
    "for column in df.columns:\n",
    "    for non_ingreedient in column.split(\"/\"):\n",
    "        if not is_ingredient(non_ingreedient):\n",
    "            non_ingreedients.append(column)\n",
    "            break\n",
    "print(\"a list of possible non ingredients\")\n",
    "print (non_ingreedients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cdc80",
   "metadata": {},
   "source": [
    "This API method doesn't work very well, so we'll sort the features manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4485e",
   "metadata": {},
   "source": [
    "remove non_ingredients from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "2340ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ingredients = [\n",
    "    'fat',               \n",
    "    'sodium',            \n",
    "    'boil',              \n",
    "    'braise',            \n",
    "    'broil',            \n",
    "    'breakfast',        \n",
    "    'cobbler/crumble',  \n",
    "    'cookie',           \n",
    "    'cookies',        \n",
    "    'fat free',        \n",
    "    'kosher',      \n",
    "    'low fat',          \n",
    "    'low/no sugar',    \n",
    "    'no sugar added',   \n",
    "    'no-cook',     \n",
    "    'raw',             \n",
    "    'stuffing/dressing',\n",
    "    'summer',       \n",
    "    'winter',         \n",
    "    'wheat/gluten-free'\n",
    "]\n",
    "for value in non_ingredients:\n",
    "    if value in features:\n",
    "        if value not in df.columns:\n",
    "            print(f\"problem with: {value}\")\n",
    "        features.remove(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d8990",
   "metadata": {},
   "source": [
    "add actually ingredients from dataset to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "90e34385",
   "metadata": {},
   "outputs": [],
   "source": [
    "actually_ingredients=actually_ingredients = [\n",
    "    'anchovy', 'anise', 'apple juice', 'artichoke',\n",
    "    'asian pear', 'basil', 'bass', 'beef rib',\n",
    "    'beef shank', 'beer', 'bell pepper', 'berry',\n",
    "    'bitters', 'blue cheese', 'brie', 'brisket',\n",
    "    'broccoli rabe', 'brown rice', 'brownie', 'butternut squash',\n",
    "    'butterscotch/caramel', 'cake', 'calvados', 'capers',\n",
    "    'caraway', 'cardamom', 'caviar', 'chard',\n",
    "    'cheese', 'chile', 'chile pepper', 'chili',\n",
    "    'chive', 'chocolate', 'cilantro', 'cinnamon',\n",
    "    'citrus', 'clam', 'clove', 'coffee',\n",
    "    'collard greens', 'cornmeal', 'couscous', 'cranberry sauce',\n",
    "    'crêpe', 'currant', 'curry', 'custard',\n",
    "    'date', 'dried fruit', 'duck', 'egg nog',\n",
    "    'endive', 'escarole', 'fennel', 'flat bread',\n",
    "    'fontina', 'fruit juice', 'ginger', 'goat cheese',\n",
    "    'goose', 'gouda', 'granola', 'green bean',\n",
    "    'ground beef', 'ground lamb', 'guava', 'halibut',\n",
    "    'hamburger', 'herb', 'hominy/cornmeal/masa', 'honey',\n",
    "    'horseradish', 'ice cream', 'iced coffee', 'iced tea',\n",
    "    'jalapeño', 'jam or jelly', 'jerusalem artichoke', 'jícama',\n",
    "    'kumquat', 'lamb chop', 'lamb shank', 'lasagna',\n",
    "    'leafy green', 'legume', 'lemon', 'lemon juice',\n",
    "    'lemongrass', 'lima bean', 'lime', 'lime juice',\n",
    "    'lingonberry', 'lobster', 'macaroni and cheese', 'maple syrup',\n",
    "    'mayonnaise', 'meatball', 'meatloaf', 'milk/cream',\n",
    "    'mint', 'molasses', 'muffin', 'mussel',\n",
    "    'mustard greens', 'noodle', 'nutmeg', 'octopus',\n",
    "    'okra', 'omelet', 'oregano', 'orzo',\n",
    "    'papaya', 'paprika', 'parsley', 'parsnip',\n",
    "    'passion fruit', 'persimmon', 'phyllo/puff pastry dough', 'poblano',\n",
    "    'pomegranate juice', 'poppy', 'pork rib', 'pork tenderloin',\n",
    "    'pot pie', 'potato salad', 'prosciutto', 'quail',\n",
    "    'quiche', 'quince', 'rabbit', 'rack of lamb',\n",
    "    'radicchio', 'radish', 'rhubarb', 'rosemary',\n",
    "    'rosé', 'rum', 'saffron', 'sage',\n",
    "    'sake', 'salad dressing', 'sardine', 'scallop',\n",
    "    'scotch', 'sesame oil', 'shellfish', 'sherry',\n",
    "    'snapper', 'sorbet', 'soufflé/meringue', 'soup/stew',\n",
    "    'sour cream', 'sourdough', 'soy sauce', 'spice',\n",
    "    'squid', 'stock', 'stuffing/dressing', 'sugar snap pea',\n",
    "    'sweet potato/yam', 'swiss cheese', 'swordfish', 'tamarind',\n",
    "    'tangerine', 'tapioca', 'tarragon', 'tea',\n",
    "    'tofu', 'tortillas', 'tree nut', 'triple sec',\n",
    "    'tropical fruit', 'trout', 'turnip', 'vanilla',\n",
    "    'veal', 'venison', 'vermouth', 'vinegar',\n",
    "    'waffle', 'wasabi', 'watercress', 'watermelon',\n",
    "    'white wine', 'wine', 'yellow squash', 'yuca'\n",
    "]\n",
    "for value in actually_ingredients:\n",
    "    if value not in features:\n",
    "        if value not in df.columns:\n",
    "            print(f\"problem with: {value}\")\n",
    "        features.append(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436650ff",
   "metadata": {},
   "source": [
    "## lets sort columns in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "69d2e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[features]\n",
    "y=df['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "d268e6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20052, 322)\n",
      "(20052,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "onion         2238.0\n",
       "tomato        2140.0\n",
       "milk/cream    1995.0\n",
       "egg           1768.0\n",
       "herb          1681.0\n",
       "               ...  \n",
       "waffle           1.0\n",
       "crêpe            1.0\n",
       "quiche           1.0\n",
       "sorbet           1.0\n",
       "sourdough        1.0\n",
       "Length: 322, dtype: float64"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape) \n",
    "print(y.shape) \n",
    "\n",
    "X.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab26982",
   "metadata": {},
   "source": [
    "## Let's start tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a6c5f8",
   "metadata": {},
   "source": [
    "это чисто препроцессинг, то что я сделал выше мб оформлю его так же шобы в пайплайн запихать "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "cfa9dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd1f082",
   "metadata": {},
   "source": [
    "#### Train-test-validation split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "5d574bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rating_to_сat(y):\n",
    "#     bins = [0, 2, 4, 6]\n",
    "#     labels = ['bad', 'ok', 'good']\n",
    "#     return pd.cut(y, bins=bins, labels=labels)\n",
    "\n",
    "class TrainValidationTest(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, test_size=0.2, random_state=21, stratified=True):\n",
    "            self.test_size = test_size\n",
    "            self.random_state = random_state\n",
    "            self.stratified = stratified\n",
    "        def split(self, X , y):\n",
    "            stratify = y if self.stratified else None\n",
    "            X_train, X_test, y_train, y_test =train_test_split(\n",
    "                X, y, test_size=self.test_size, random_state=self.random_state, stratify=stratify\n",
    "            )\n",
    "            X_train, X_valid, y_train, y_valid =train_test_split(\n",
    "                X_train, y_train, test_size=self.test_size/(1-self.test_size), random_state=self.random_state, stratify=stratify\n",
    "            )\n",
    "            return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5918900",
   "metadata": {},
   "source": [
    "#### Model selection class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "0edc3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from itertools import product\n",
    "from sklearn.metrics import get_scorer\n",
    "def manual_grid_search(X_train, y_train, X_valid, y_valid, model_estimator,param_grid, cv, scoring='accuracy_score',  n_jobs=-1, error_score=np.nan):\n",
    "    res=[]\n",
    "    estimator=deepcopy(model_estimator)\n",
    "    total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "    # get scorer for valid score\n",
    "    scorer = get_scorer(scoring)\n",
    "\n",
    "    for params in tqdm(list(product(*param_grid.values())), total=total_combinations, desc=\"GridSearch\"):\n",
    "\n",
    "        current_model = deepcopy(estimator)\n",
    "        current_model.set_params(**(dict(zip(param_grid.keys(), params))))\n",
    "        try:\n",
    "            score=cross_val_score(\n",
    "                current_model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=n_jobs,error_score=error_score\n",
    "            )\n",
    "\n",
    "            current_model.fit(X_train, y_train)\n",
    "            valid_score=scorer(current_model, X_valid, y_valid)\n",
    "\n",
    "            res.append({\n",
    "                'train_score': score.mean(),\n",
    "                'valid_score': valid_score,\n",
    "                'params': dict(zip(param_grid.keys(), params))\n",
    "            })\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with params {params}: {str(e)}\")\n",
    "            res.append({\n",
    "                **(dict(zip(param_grid.keys(), params))),\n",
    "                'train_score': 'Na',\n",
    "                'valid_score': 'Na'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "    return pd.DataFrame(res).sort_values('valid_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "979d1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelection:\n",
    "    def __init__ (self,estimators,estimators_name, grid_dict, scoring='accuracy_score'):\n",
    "        self.estimators=estimators\n",
    "        self.scoring=scoring\n",
    "        self.estimators_name=estimators_name\n",
    "        self.grid_dict=grid_dict\n",
    "        self.results=pd.DataFrame()\n",
    "        self.best_res=[]\n",
    "    def choose(self,X_train, y_train, X_valid, y_valid, cv=2):\n",
    "        \"\"\"\n",
    "        Method choose() takes X_train, y_train, X_valid, y_valid, optional cv, \n",
    "        and returns the name of the best model among all the models on the validation set\n",
    "        \"\"\"\n",
    "        for estimator in self.estimators:\n",
    "            model_name= self.estimators_name[self.estimators.index(estimator)]\n",
    "            search_res=manual_grid_search(X_train,y_train,X_valid, y_valid,estimator,self.grid_dict[model_name], cv=cv, scoring=self.scoring)\n",
    "            search_res['model']=model_name\n",
    "            best_params = search_res.head(1)['params'].item()\n",
    "            best_train_score = search_res.head(1)['train_score'].item()\n",
    "            best_valid_score = search_res.head(1)['valid_score'].item()\n",
    "            self.results=pd.concat([self.results, search_res], axis=0, ignore_index=True)\n",
    "            self.best_res.append({\n",
    "                'model': model_name,\n",
    "                'params': best_params,\n",
    "                'valid_score': best_valid_score,\n",
    "                'train_score': best_train_score\n",
    "            })\n",
    "            \n",
    "            print(f\"Best params for model{model_name}: {best_params}\")\n",
    "            print(f\"training accuracy: {best_train_score:.3f}\")\n",
    "            print(f\"Best on validation set accuracy score: {best_valid_score:.3f}\")\n",
    "        self.best_res=pd.DataFrame(self.best_res)\n",
    "        self.best_res.sort_values('valid_score', ascending=False,inplace=True)\n",
    "        best_model_name=self.best_res.head(1)['model'].item()\n",
    "        print(f\"\\nBest model: {best_model_name}\\nwith params: {self.best_res.head(1)['params'].item()}\\nand score: {self.best_res.head(1)['valid_score'].item()}\")\n",
    "        return self.results.sort_values('valid_score', ascending=False)\n",
    "    # def show_results(self):\n",
    "        # return self.results.groupby(by=\"model\").sort_values('valid_score', ascending=False)\n",
    "    def fit_score(self,X_comb,y_comb, X_test, y_test):\n",
    "        params=self.best_res.head(1)['params'].item()\n",
    "        model_name=self.best_res.head(1)['model'].item()\n",
    "        print(f'Taking model: {model_name} with params:{params}')\n",
    "        best_estimator=self.estimators[self.estimators_name.index(model_name)]\n",
    "        best_estimator.set_params(**(params))\n",
    "        best_estimator.fit(X_comb,y_comb)\n",
    "        scorer=get_scorer(self.scoring)\n",
    "        print(\"the final score is :\",(scorer(best_estimator,X_test,y_test)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "25056450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_parametrs(X,y,estimators,estimators_name, grid_dict, scoring='accuracy_score'):\n",
    "    splitter = TrainValidationTest(stratified=False)\n",
    "    X_train, X_valid, X_test, y_train, y_valid, y_test = splitter.split(X, y)\n",
    "    search_parametrs= ModelSelection(estimators,estimators_name, grid_dict, scoring)\n",
    "    result_serching_df=search_parametrs.choose(X_train, y_train, X_valid, y_valid)\n",
    "    X_comb=pd.concat([X_train, X_valid], axis=0, ignore_index=True)\n",
    "    y_comb=pd.concat([y_train, y_valid], axis=0, ignore_index=True)\n",
    "    print(\"Calc accuracy on the best model\")\n",
    "    search_parametrs.fit_score(X_comb,y_comb, X_test,y_test)\n",
    "    return(result_serching_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d895e4",
   "metadata": {},
   "source": [
    "# Regression serch params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee08122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "estimators = [\n",
    "    LinearRegression(),\n",
    "    Ridge(),\n",
    "    Lasso(),\n",
    "    ElasticNet(),\n",
    "    RandomForestRegressor(random_state=21),\n",
    "    GradientBoostingRegressor(random_state=21),\n",
    "    SVR(),\n",
    "    KNeighborsRegressor(),\n",
    "    KernelRidge(),\n",
    "]\n",
    "\n",
    "estimators_name = [\n",
    "    'LinearRegression',\n",
    "    'Ridge',\n",
    "    'Lasso',\n",
    "    'ElasticNet',\n",
    "    'RandomForest',\n",
    "    'GradientBoosting',\n",
    "    'SVR',\n",
    "    'KNN',\n",
    "    'KernelRidge',\n",
    "]\n",
    "\n",
    "grid_dict = {\n",
    "    'LinearRegression': {\n",
    "        'fit_intercept': [True, False],\n",
    "        'copy_X': [True, False],\n",
    "        'positive': [True, False],\n",
    "        'n_jobs': [-1]\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "        'fit_intercept': [True, False],\n",
    "        'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        'fit_intercept': [True, False],\n",
    "        'selection': ['cyclic', 'random'],\n",
    "        'positive': [True, False]\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "        'fit_intercept': [True, False],\n",
    "        'selection': ['cyclic', 'random']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 5, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'epsilon': [0.01, 0.1, 0.5],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'leaf_size': [10, 30, 50],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "    'KernelRidge': {\n",
    "        'alpha': [0.1, 1.0, 10.0],\n",
    "        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'gamma': [None, 0.1, 1.0],\n",
    "        'degree': [2, 3, 4]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'reg_alpha': [0, 0.1, 1],\n",
    "        'reg_lambda': [0, 0.1, 1]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'num_leaves': [15, 31, 63],\n",
    "        'max_depth': [-1, 5, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'min_child_samples': [5, 10, 20],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 1],\n",
    "        'reg_lambda': [0, 0.1, 1]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [50, 100, 200],\n",
    "        'depth': [4, 6, 8],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'l2_leaf_reg': [1, 3, 5],\n",
    "        'border_count': [32, 64, 128],\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "ccb8f92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fb5b67dad9435eb4c40c6ba0189734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridSearch:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for modelLinearRegression: {'fit_intercept': True, 'copy_X': True, 'positive': False, 'n_jobs': -1}\n",
      "training accuracy: -1.339\n",
      "Best on validation set accuracy score: -1.292\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487070621fe94be3a1bb4e4f0923b252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridSearch:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for modelRidge: {'alpha': 100.0, 'fit_intercept': True, 'solver': 'saga'}\n",
      "training accuracy: -1.325\n",
      "Best on validation set accuracy score: -1.308\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1612134a83a841a7b6fa1428182e3c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridSearch:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for modelLasso: {'alpha': 0.001, 'fit_intercept': True, 'selection': 'cyclic', 'positive': False}\n",
      "training accuracy: -1.320\n",
      "Best on validation set accuracy score: -1.299\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e52c2e006334de68c7e38425258feb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridSearch:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for modelElasticNet: {'alpha': 0.01, 'l1_ratio': 0.3, 'fit_intercept': True, 'selection': 'random'}\n",
      "training accuracy: -1.332\n",
      "Best on validation set accuracy score: -1.300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1325f35b82e1443091aee54c376c899b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridSearch:   0%|          | 0/648 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for modelRandomForest: {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True}\n",
      "training accuracy: -1.302\n",
      "Best on validation set accuracy score: -1.266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddce41fa359649f0b5413652ba9f4eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridSearch:   0%|          | 0/432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for modelGradientBoosting: {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'log2', 'subsample': 0.8}\n",
      "training accuracy: -1.319\n",
      "Best on validation set accuracy score: -1.275\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2a29f7ac194487be41aee3dbeb2e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridSearch:   0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for modelSVR: {'kernel': 'rbf', 'C': 1.0, 'epsilon': 0.5, 'gamma': 'scale'}\n",
      "training accuracy: -1.336\n",
      "Best on validation set accuracy score: -1.303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4833a21836454c3e8239625bc786ba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridSearch:   0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for modelKNN: {'n_neighbors': 9, 'weights': 'distance', 'algorithm': 'kd_tree', 'leaf_size': 30, 'p': 2}\n",
      "training accuracy: -1.427\n",
      "Best on validation set accuracy score: -1.342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "629a316532ea4b69bee040396007b9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridSearch:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n",
      "/home/chumba/projects/DS_Bootcamp.Team01.ID_886525-Team_TL_shirledo.396ad497_d1b1_4d29-1/src/venv/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:254: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for modelKernelRidge: {'alpha': 10.0, 'kernel': 'poly', 'gamma': 0.1, 'degree': 4}\n",
      "training accuracy: -1.315\n",
      "Best on validation set accuracy score: -1.300\n",
      "\n",
      "Best model: RandomForest\n",
      "with params: {'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True}\n",
      "and score: -1.2655243055301424\n",
      "Calc accuracy on the best model\n",
      "Taking model: RandomForest with params:{'n_estimators': 50, 'max_depth': None, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'bootstrap': True}\n",
      "the final score is : -1.287821723812737\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>valid_score</th>\n",
       "      <th>params</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>-1.301510</td>\n",
       "      <td>-1.265524</td>\n",
       "      <td>{'n_estimators': 50, 'max_depth': None, 'min_s...</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>-1.299593</td>\n",
       "      <td>-1.265561</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': None, 'min_...</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>-1.299339</td>\n",
       "      <td>-1.265718</td>\n",
       "      <td>{'n_estimators': 200, 'max_depth': None, 'min_...</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>-1.306118</td>\n",
       "      <td>-1.265953</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': None, 'min_...</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>-1.306251</td>\n",
       "      <td>-1.266002</td>\n",
       "      <td>{'n_estimators': 200, 'max_depth': None, 'min_...</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>-131.683225</td>\n",
       "      <td>-59.327460</td>\n",
       "      <td>{'alpha': 1.0, 'kernel': 'sigmoid', 'gamma': 1...</td>\n",
       "      <td>KernelRidge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>-131.683225</td>\n",
       "      <td>-59.327460</td>\n",
       "      <td>{'alpha': 1.0, 'kernel': 'sigmoid', 'gamma': 1...</td>\n",
       "      <td>KernelRidge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>-90.714581</td>\n",
       "      <td>-171.905865</td>\n",
       "      <td>{'alpha': 0.1, 'kernel': 'sigmoid', 'gamma': 1...</td>\n",
       "      <td>KernelRidge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>-90.714581</td>\n",
       "      <td>-171.905865</td>\n",
       "      <td>{'alpha': 0.1, 'kernel': 'sigmoid', 'gamma': 1...</td>\n",
       "      <td>KernelRidge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>-90.714581</td>\n",
       "      <td>-171.905865</td>\n",
       "      <td>{'alpha': 0.1, 'kernel': 'sigmoid', 'gamma': 1...</td>\n",
       "      <td>KernelRidge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1664 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train_score  valid_score  \\\n",
       "212     -1.301510    -1.265524   \n",
       "213     -1.299593    -1.265561   \n",
       "214     -1.299339    -1.265718   \n",
       "215     -1.306118    -1.265953   \n",
       "216     -1.306251    -1.266002   \n",
       "...           ...          ...   \n",
       "1659  -131.683225   -59.327460   \n",
       "1660  -131.683225   -59.327460   \n",
       "1661   -90.714581  -171.905865   \n",
       "1663   -90.714581  -171.905865   \n",
       "1662   -90.714581  -171.905865   \n",
       "\n",
       "                                                 params         model  \n",
       "212   {'n_estimators': 50, 'max_depth': None, 'min_s...  RandomForest  \n",
       "213   {'n_estimators': 100, 'max_depth': None, 'min_...  RandomForest  \n",
       "214   {'n_estimators': 200, 'max_depth': None, 'min_...  RandomForest  \n",
       "215   {'n_estimators': 100, 'max_depth': None, 'min_...  RandomForest  \n",
       "216   {'n_estimators': 200, 'max_depth': None, 'min_...  RandomForest  \n",
       "...                                                 ...           ...  \n",
       "1659  {'alpha': 1.0, 'kernel': 'sigmoid', 'gamma': 1...   KernelRidge  \n",
       "1660  {'alpha': 1.0, 'kernel': 'sigmoid', 'gamma': 1...   KernelRidge  \n",
       "1661  {'alpha': 0.1, 'kernel': 'sigmoid', 'gamma': 1...   KernelRidge  \n",
       "1663  {'alpha': 0.1, 'kernel': 'sigmoid', 'gamma': 1...   KernelRidge  \n",
       "1662  {'alpha': 0.1, 'kernel': 'sigmoid', 'gamma': 1...   KernelRidge  \n",
       "\n",
       "[1664 rows x 4 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_parametrs(X,y,estimators,estimators_name, grid_dict, scoring='neg_root_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e96dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
